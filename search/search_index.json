{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#bolt","title":"Bolt","text":""},{"location":"#bendable-optimization-and-learning-toolkit","title":"Bendable Optimization and Learning Toolkit","text":"<p>Bolt is the fastest man alive and a cool cartoon dog, but here we bring to you a toolkit to easily run machine learning algorithms. Bolt is a Bend module for machine learning and is distributed under the 3-Clause BSD license.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#dependencies","title":"Dependencies","text":"<p>Bolt requires: - Bend (&gt;= 0.2.37) - HVM (&gt;= 2.0.22)  </p> <p>Visit official bends repositor for instructions on how to install bend</p>"},{"location":"#user-installation","title":"User installation","text":"<p>Since bend does not have a package manager yet, clone the repo and you can directly import from bolt directory. <pre><code>git clone https://github.com/InferBend/Bolt.git\n</code></pre></p>"},{"location":"About%20Us/Contributors/","title":"Contributor","text":"<p>Bolt was started as a passion project by us to leverage out the potential of Bend and see its application in the field of ML.</p>"},{"location":"About%20Us/Contributors/#core-team","title":"Core team","text":"<ul> <li>@Nihal-Srivastava05</li> <li>@Jaisuraj</li> <li>@amaljohn2304</li> </ul>"},{"location":"About%20Us/Contributors/#design-team","title":"Design team","text":"<ul> <li>@anuraagvad - @vxddoesthings on insta</li> </ul>"},{"location":"linear_model/LinearRegression/","title":"LinearRegression","text":"<p>bolt &gt; linear_model &gt; _linearRegression</p>"},{"location":"linear_model/LinearRegression/#linearregression","title":"LinearRegression","text":"<p>The <code>LinearRegression</code> implements a linear regression model trained using the gradient descent algorithm. This model finds the optimal linear relationship between input features and target values by iteratively minimizing the error between predictions and actual values.</p>"},{"location":"linear_model/LinearRegression/#overview","title":"Overview","text":"<p>Linear Regression with gradient descent is a foundational machine learning algorithm that fits a straight line (or hyperplane) to the data by adjusting model weights iteratively. Gradient descent is used here to minimize the Mean Squared Error (MSE) loss, making it suitable for large datasets and continuous-valued outputs.</p>"},{"location":"linear_model/LinearRegression/#parameters","title":"Parameters","text":""},{"location":"linear_model/LinearRegression/#linearregressionfitx-listlistf24-y-listf24-max_iter-u24-learning_rate-f24-tol-f24","title":"<code>LinearRegression/fit(x: List[List[f24]], y: List[f24], max_iter: u24, learning_rate: f24, tol: f24)</code>","text":"<ul> <li>x: Training data, a list of lists where each sublist represents a feature vector of type <code>f24</code>.</li> <li>y: Target values, a list of continuous values corresponding to each feature vector in <code>x</code>, type <code>f24</code>.</li> <li>max_iter: Maximum number of iterations to run the gradient descent algorithm, type <code>u24</code>.</li> <li>learning_rate: Step size for each iteration of gradient descent, determining how much the weights are adjusted per step, type <code>f24</code>.</li> <li>tol: Tolerance for convergence. If the change in the loss is smaller than <code>tol</code>, gradient descent will stop early, type <code>f24</code>.</li> </ul> <p>Returns: A fitted <code>LinearRegression</code> model.</p>"},{"location":"linear_model/LinearRegression/#linearregressionpredictmodel-linearregression-x-listlistf24","title":"<code>LinearRegression/predict(model: LinearRegression, x: List[List[f24]])</code>","text":"<ul> <li>model: A fitted <code>LinearRegression</code> instance.</li> <li>x: Input data for prediction, a list of lists where each sublist represents a feature vector, type <code>f24</code>.</li> </ul> <p>Returns: Predicted continuous values as an array, with each element corresponding to a prediction for each data sample in <code>x</code>.</p>"},{"location":"linear_model/LinearRegression/#model-attributes","title":"Model Attributes","text":"<p>Once the model is trained using LinearRegression/fit, it has the following attributes:</p> <ul> <li>weights: A list of coefficients (one per feature) that define the slope of the line (or hyperplane) fitted to the data, type f24. These weights are adjusted by gradient descent to minimize error.</li> <li>bias: A single float value representing the intercept, or the value at which the fitted line crosses the y-axis when all feature values are zero. The bias term is also updated during training.</li> <li>cost: List of Mean Squared Error (MSE) value achieved by the model after each epoch. This value reflects the degree of error in the model's predictions.</li> </ul>"},{"location":"linear_model/LinearRegression/#examples","title":"Examples","text":""},{"location":"linear_model/LinearRegression/#basic-usage","title":"Basic Usage","text":"<pre><code># Training data\nx = [\n    [7.0, 99.0, 9.0],\n    [4.0, 82.0, 4.0],\n    [8.0, 51.0, 7.0],\n    [5.0, 52.0, 5.0],\n    [7.0, 75.0, 8.0],\n    [3.0, 78.0, 9.0],\n]\ny = [91.0, 68.777, 45.0, 36.0, 66.0, 61.0]\n\n# Gradient descent parameters\nmax_iter = 100\nlearning_rate = 0.0001\ntol = 0.0001\n\n# Fit the Linear Regression model\nmodel = LinearRegression/fit(x, y, max_iter, learning_rate, tol)\n\n# Define test data for prediction\nxn = [\n    [9.0, 100.0, 3.0],\n    [1.0, 60.0, 7.0]\n]\n\n# Predict target values for test data\ny_pred = LinearRegression/predict(model, xn)\n# Output: Predicted Values: [predicted_value1, predicted_value2]\n</code></pre>"},{"location":"linear_model/LinearRegression/#accessing-model-attributes","title":"Accessing Model Attributes","text":"<p>After fitting the model, you can access these attributes to understand the learned parameters:</p> <pre><code>model = LinearRegression/fit(x, y, max_iter, learning_rate, tol)\n\n# Accessing model weights, bias, and cost\nmatch model:\n  case LinearRegression/Model:\n    IO/print(model.weights)\n    IO/print(model.bias)\n    IO/print(model.cost)\n</code></pre>"},{"location":"linear_model/LinearRegression/#explanation-of-gradient-descent-parameters","title":"Explanation of Gradient Descent Parameters","text":"<ul> <li>max_iter: Increasing <code>max_iter</code> can improve accuracy but may slow down training.</li> <li>learning_rate: Too high of a learning rate might lead to divergence, while too low of a learning rate could result in slow convergence.</li> <li>tol: Useful for setting an early stopping criterion, speeding up training when the error improvement becomes minimal.</li> </ul>"},{"location":"linear_model/LinearRegression/#common-issues-and-error-handling","title":"Common Issues and Error Handling","text":"<ul> <li>Learning Rate Too High: If the learning rate is too high, gradient descent may diverge. Start with a small learning rate and adjust as needed.</li> <li>Insufficient <code>max_iter</code>: If the model doesn't converge within the specified iterations, try increasing <code>max_iter</code> or adjusting the <code>learning_rate</code>.</li> <li>Imbalanced Features: Standardize or normalize features if there are large variations in feature scales, as this improves gradient descent performance.</li> </ul>"},{"location":"linear_model/LogisticRegression/","title":"LogisticRegression","text":"<p>bolt &gt; linear_models &gt; _logisticRegression</p>"},{"location":"linear_model/LogisticRegression/#logisticregression","title":"LogisticRegression","text":"<p>The <code>LogisticRegression</code> class implements a logistic regression model trained using gradient descent. This model is designed for binary classification tasks, predicting the probability that a given input belongs to a specific class by learning the optimal decision boundary between the classes.</p>"},{"location":"linear_model/LogisticRegression/#overview","title":"Overview","text":"<p>Logistic Regression is a widely used statistical method for binary classification that outputs probabilities for each class. The model applies the sigmoid function to transform linear predictions into probabilities, making it ideal for predicting binary outcomes.</p>"},{"location":"linear_model/LogisticRegression/#parameters","title":"Parameters","text":""},{"location":"linear_model/LogisticRegression/#logisticregressionfitx-listlistf24-y-listf24-max_iter-u24-learning_rate-f24-tol-f24","title":"<code>LogisticRegression/fit(x: List[List[f24]], y: List[f24], max_iter: u24, learning_rate: f24, tol: f24)</code>","text":"<ul> <li>x: Training data, a list of lists where each sublist represents a feature vector of type <code>f24</code>.</li> <li>y: Target values, a list of binary values (e.g., 0 or 1) corresponding to each feature vector in <code>x</code>, type <code>f24</code>.</li> <li>max_iter: Maximum number of iterations to run the gradient descent algorithm, type <code>u24</code>.</li> <li>learning_rate: Step size for each iteration of gradient descent, determining the magnitude of updates to the weights and bias, type <code>f24</code>.</li> <li>tol: Tolerance for convergence; if the change in loss is less than <code>tol</code>, the algorithm stops early, type <code>f24</code>.</li> </ul> <p>Returns: A fitted <code>LogisticRegression</code> model.</p>"},{"location":"linear_model/LogisticRegression/#logisticregressionpredictmodel-logisticregression-x-listlistf24","title":"<code>LogisticRegression/predict(model: LogisticRegression, x: List[List[f24]])</code>","text":"<ul> <li>model: A fitted <code>LogisticRegression</code> instance.</li> <li>x: Input data for prediction, a list of lists where each sublist represents a feature vector, type <code>f24</code>.</li> </ul> <p>Returns: Predicted class labels (0 or 1) for each data sample in <code>x</code>.</p>"},{"location":"linear_model/LogisticRegression/#model-attributes","title":"Model Attributes","text":"<p>After fitting the model using <code>LogisticRegression/fit</code>, the following attributes become available:</p> <ul> <li>weights: A list of coefficients (one per feature) that define the slope of the decision boundary for classification, type <code>f24</code>. These weights are optimized by gradient descent.</li> <li>bias: A single float value representing the intercept of the decision boundary, type <code>f24</code>.</li> <li>cost: The final Binary Cross-Entropy loss (cost) achieved by the model after training. This value reflects the degree of error in the model's predictions.</li> </ul>"},{"location":"linear_model/LogisticRegression/#accessing-model-attributes","title":"Accessing Model Attributes","text":"<p>After training, you can inspect the model\u2019s learned parameters:</p> <pre><code>model = LogisticRegression/fit(x, y, max_iter, learning_rate, tol)\n\n# Accessing model weights, bias, and cost\nmatch model:\n  case LogisticRegression/Model:\n    IO/print(model.weights)\n    IO/print(model.bias)\n    IO/print(model.cost)\n</code></pre>"},{"location":"linear_model/LogisticRegression/#examples","title":"Examples","text":""},{"location":"linear_model/LogisticRegression/#basic-usage","title":"Basic Usage","text":"<pre><code># Training data\nx = [\n    [7.0, 2.0],\n    [3.0, 4.0],\n    [1.0, 6.0],\n    [6.0, 1.0],\n    [3.0, 5.0],\n    [4.0, 2.0],\n]\ny = [1.0, 0.0, 1.0, 0.0, 1.0, 0.0]\n\n# Gradient descent parameters\nmax_iter = 100\nlearning_rate = 0.01\ntol = 0.001\n\n# Fit the Logistic Regression model\nmodel = LogisticRegression/fit(x, y, max_iter, learning_rate, tol)\n\n# Define test data for prediction\nxn = [\n    [5.0, 3.0],\n    [2.0, 7.0]\n]\n\n# Predict class labels for test data\ny_pred = LogisticRegression/predict(model, xn)\n# Output: Predicted Class Labels: [0.0, 1.0]\n</code></pre>"},{"location":"linear_model/LogisticRegression/#common-issues-and-error-handling","title":"Common Issues and Error Handling","text":"<ul> <li>Divergence: If the learning rate is too high, gradient descent may not converge. Try a smaller learning rate.</li> <li>Non-convergence within max_iter: If the model doesn\u2019t converge, increase <code>max_iter</code> or adjust <code>learning_rate</code>.</li> <li>Feature Scaling: Standardize or normalize features to improve gradient descent efficiency, especially with large variations in feature scales.</li> </ul>"},{"location":"metrics/Classification/","title":"Classification","text":"<p>bolt &gt; metrics &gt; _Classification</p>"},{"location":"metrics/Classification/#classification","title":"Classification","text":"<p>These metrics provide essential tools for evaluating the performance of classification models.</p>"},{"location":"metrics/Classification/#accuracy_score","title":"accuracy_score","text":"<p>The <code>accuracy_score</code> metric calculates the accuracy of a classification model by comparing the predicted labels to the true labels. It\u2019s defined as the ratio of the number of correct predictions to the total number of predictions, making it a commonly used metric for evaluating classification performance.</p>"},{"location":"metrics/Classification/#overview","title":"Overview","text":"<p>Accuracy is a simple but effective metric, especially for balanced datasets where classes are evenly represented. It\u2019s calculated as:</p> \\[ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} \\] <p>This function is ideal for quick model evaluation and gives an immediate sense of how well the model performs on a given dataset.</p>"},{"location":"metrics/Classification/#parameters","title":"Parameters","text":""},{"location":"metrics/Classification/#accuracy_scoreytrue-listf24-ypred-listf24","title":"<code>accuracy_score(yTrue: List[f24], yPred: List[f24])</code>","text":"<ul> <li>yTrue: List of true target values, with each element representing the correct label for a corresponding sample, type <code>f24</code>.</li> <li>yPred: List of predicted target values, with each element representing the label predicted by the model for a corresponding sample, type <code>f24</code>.</li> </ul> <p>Returns: The accuracy of the model, calculated as a float between <code>0.0</code> (no correct predictions) and <code>1.0</code> (all predictions correct).</p>"},{"location":"metrics/Classification/#examples","title":"Examples","text":""},{"location":"metrics/Classification/#basic-usage","title":"Basic Usage","text":"<pre><code># True labels\ny_true = [0.0, 1.0, 0.0, 1.0]\n\n# Predicted labels\ny_pred = [0.0, 1.0, 1.0, 1.0]\n\n# Calculate accuracy\naccuracy = accuracy_score(y_true, y_pred)\n# Output: accuracy: 0.75\n</code></pre>"},{"location":"metrics/Classification/#common-issues-and-error-handling","title":"Common Issues and Error Handling","text":"<ul> <li>Mismatched Lengths: Ensure <code>yTrue</code> and <code>yPred</code> have the same length, as the function requires one-to-one correspondence between true and predicted labels.</li> <li>Imbalanced Classes: On highly imbalanced datasets, accuracy might be misleading. Consider additional metrics, such as precision, recall, or F1 score, for a more comprehensive evaluation.</li> </ul>"},{"location":"metrics/Regression/","title":"Regression","text":"<p>bolt &gt; metrics &gt; _Regression</p>"},{"location":"metrics/Regression/#regression-metrics","title":"Regression Metrics","text":"<p>These metrics provide essential tools for evaluating the performance of regression models. Each metric measures error or fit quality differently, offering unique insights into model performance.</p>"},{"location":"metrics/Regression/#meansquarederror","title":"meanSquaredError","text":"<p>The <code>meanSquaredError</code> (MSE) metric calculates the average squared difference between predicted and actual values, making it useful for measuring the magnitude of prediction errors. Larger errors are penalized more heavily due to the squaring, making this metric sensitive to outliers.</p>"},{"location":"metrics/Regression/#parameters","title":"Parameters","text":"<ul> <li>yTrue: List of true target values, each representing the correct continuous value for a corresponding sample, type <code>f24</code>.</li> <li>yPred: List of predicted target values, each representing the continuous value predicted by the model for a corresponding sample, type <code>f24</code>.</li> </ul> <p>Returns: The mean squared error as a float.</p>"},{"location":"metrics/Regression/#formula","title":"Formula","text":"\\[ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_{\\text{true},i} - y_{\\text{pred},i})^2 \\]"},{"location":"metrics/Regression/#example","title":"Example","text":"<pre><code>y_true = [3.0, -0.5, 2.0, 7.0]\ny_pred = [2.5, 0.0, 2.0, 8.0]\n\nmse = meanSquaredError(y_true, y_pred)\n# Output: mse: 0.375\n</code></pre>"},{"location":"metrics/Regression/#meanabsoluteerror","title":"meanAbsoluteError","text":"<p>The <code>meanAbsoluteError</code> (MAE) metric calculates the average absolute difference between predicted and actual values, providing a straightforward measure of error without penalizing larger errors as much as MSE. It\u2019s useful for models where you want to interpret errors in absolute terms.</p>"},{"location":"metrics/Regression/#parameters_1","title":"Parameters","text":"<ul> <li>yTrue: List of true target values, type <code>f24</code>.</li> <li>yPred: List of predicted target values, type <code>f24</code>.</li> </ul> <p>Returns: The mean absolute error as a float.</p>"},{"location":"metrics/Regression/#formula_1","title":"Formula","text":"\\[ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_{\\text{true},i} - y_{\\text{pred},i}| \\]"},{"location":"metrics/Regression/#example_1","title":"Example","text":"<pre><code>y_true = [3.0, -0.5, 2.0, 7.0]\ny_pred = [2.5, 0.0, 2.0, 8.0]\n\nmae = meanAbsoluteError(y_true, y_pred)\n# Output: mae: 0.5\n</code></pre>"},{"location":"metrics/Regression/#r2_score","title":"r2_score","text":"<p>The <code>r2_score</code> (R-squared) metric, also known as the coefficient of determination, indicates the proportion of variance in the dependent variable that is predictable from the independent variable(s). It ranges from <code>0.0</code> to <code>1.0</code>, with higher values indicating a better fit.</p>"},{"location":"metrics/Regression/#parameters_2","title":"Parameters","text":"<ul> <li>yTrue: List of true target values, type <code>f24</code>.</li> <li>yPred: List of predicted target values, type <code>f24</code>.</li> </ul> <p>Returns: The R-squared score as a float, where values closer to <code>1.0</code> indicate better model performance.</p>"},{"location":"metrics/Regression/#formula_2","title":"Formula","text":"\\[ R^2 = 1 - \\frac{\\text{Sum of Squared Errors (SSE)}}{\\text{Total Sum of Squares (TSS)}} \\] <p>where: - SSE is the sum of squared errors, calculated as \\(\\sum (y_{\\text{true}} - y_{\\text{pred}})^2\\). - TSS is the total sum of squares, calculated as \\(\\sum (y_{\\text{true}} - \\text{mean}(y_{\\text{true}}))^2\\).  </p>"},{"location":"metrics/Regression/#example_2","title":"Example","text":"<pre><code>y_true = [3.0, -0.5, 2.0, 7.0]\ny_pred = [2.5, 0.0, 2.0, 8.0]\n\nr2 = r2_score(y_true, y_pred)\n# Output: r2_score: 0.948\n</code></pre>"},{"location":"metrics/Regression/#totalsumofsquares","title":"totalSumOfSquares","text":"<p>The <code>totalSumOfSquares</code> (TSS) metric provides a measure of the total variance of the true target values. It serves as a baseline metric to compare against the sum of squared errors (SSE) for computing metrics like <code>r2_score</code>.</p>"},{"location":"metrics/Regression/#parameters_3","title":"Parameters","text":"<ul> <li>yTrue: List of true target values, type <code>f24</code>.</li> </ul> <p>Returns: The total sum of squares as a float.</p>"},{"location":"metrics/Regression/#formula_3","title":"Formula","text":"\\[ \\text{TSS} = \\sum_{i=1}^{n} (y_{\\text{true},i} - \\text{mean}(y_{\\text{true}}))^2 \\]"},{"location":"metrics/Regression/#example_3","title":"Example","text":"<pre><code>y_true = [3.0, -0.5, 2.0, 7.0]\n\ntss = totalSumOfSquares(y_true)\n# Output: tss: 29.25\n</code></pre>"},{"location":"metrics/Regression/#common-issues-and-error-handling","title":"Common Issues and Error Handling","text":"<ul> <li>Mismatched Lengths: Ensure <code>yTrue</code> and <code>yPred</code> are of the same length for all metrics requiring both.</li> <li>Outliers: For datasets with extreme values, consider using <code>MAE</code> over <code>MSE</code> to reduce sensitivity to outliers.</li> <li>Negative R-squared: In rare cases, <code>R^2</code> can be negative, indicating that the model performs worse than a simple mean-based prediction.</li> </ul>"},{"location":"model_selection/train_test_split/","title":"Train test split","text":"<p>bolt &gt; model_selection &gt; _train_test_split</p>"},{"location":"model_selection/train_test_split/#train_test_split","title":"train_test_split","text":"<p>The <code>train_test_split</code> function splits datasets into training and testing subsets. It provides an easy way to evaluate machine learning models on unseen data by separating a portion of the dataset for validation or testing.</p>"},{"location":"model_selection/train_test_split/#overview","title":"Overview","text":"<p>This function splits input arrays into random train and test subsets while maintaining the structure of input arrays. It allows customization of the test size, shuffling of data, and reproducibility through a random state.</p>"},{"location":"model_selection/train_test_split/#parameters","title":"Parameters","text":""},{"location":"model_selection/train_test_split/#train_test_splitarrays-list-test_size-f24-shuffle-u8-random_state-f24","title":"<code>train_test_split(arrays: List, test_size: f24, shuffle: u8, random_state: f24)</code>","text":"<ul> <li> <p>arrays: A list of arrays to be split. Each array should have the same length (number of samples). Typically includes features (<code>X</code>) and target labels (<code>y</code>).</p> </li> <li> <p>test_size: Fraction of the dataset to allocate for testing. Must be a float between 0 and 1. For example, <code>test_size = 0.25</code> reserves 25% of the data for testing.</p> </li> <li> <p>shuffle: A binary flag (<code>0</code> or <code>1</code>) indicating whether to shuffle the data before splitting. Set to <code>1</code> to shuffle the data and <code>0</code> to retain the original order.</p> </li> <li> <p>random_state: A float seed value used for random number generation. Ensures reproducibility of the shuffle when <code>shuffle</code> is enabled. The same <code>random_state</code> will produce the same train-test split across runs.</p> </li> </ul> <p>Returns: A list of arrays split into train and test subsets. The order of the arrays matches the input, with the first half of the arrays corresponding to the training set and the second half to the testing set.</p>"},{"location":"model_selection/train_test_split/#examples","title":"Examples","text":""},{"location":"model_selection/train_test_split/#splitting-a-dataset","title":"Splitting a Dataset","text":"<pre><code># Input feature matrix and target labels\nX = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0], [13.0, 14.0, 15.0]]\ny = [1.0, 2.0, 3.0, 4.0, 5.0]\n\n# Split data with 25% for testing, shuffled, and a fixed random state\ndataset = train_test_split([X, y], 0.25, 1, 1.0)\n\n# Extract training and testing subsets\nX_train, X_test = List/index(dataset, 0)\ny_train, y_test = List/index(dataset, 1)\n\n# Results\nwith IO:\n    IO/print(\"Training Features:\", X_train)\n    IO/print(\"Testing Features:\", X_test)\n    IO/print(\"Training Labels:\", y_train)\n    IO/print(\"Testing Labels:\", y_test)\n</code></pre>"},{"location":"model_selection/train_test_split/#explanation-of-parameters","title":"Explanation of Parameters","text":"<ol> <li>Test Size: Defines the proportion of the data to reserve for testing. For example:</li> <li><code>test_size = 0.2</code>: 80% training, 20% testing.</li> <li> <p><code>test_size = 0.5</code>: 50% training, 50% testing.</p> </li> <li> <p>Shuffle: If set to <code>1</code>, the data is shuffled randomly before splitting. If <code>0</code>, the data remains in its original order.</p> </li> <li> <p>Random State: Ensures consistent splits when <code>shuffle</code> is enabled. For instance, using <code>random_state = 1.0</code> will always produce the same shuffle and split.</p> </li> </ol>"},{"location":"model_selection/train_test_split/#common-issues-and-error-handling","title":"Common Issues and Error Handling","text":"<ol> <li> <p>Mismatched Array Lengths: Ensure that all arrays in the <code>arrays</code> parameter have the same number of samples. Mismatched lengths will raise an error.</p> </li> <li> <p>Test Size Range: <code>test_size</code> must be a float between <code>0</code> and <code>1</code>. Values outside this range will result in an error.</p> </li> <li> <p>Shuffle Disabled with Random State: If <code>shuffle = 0</code>, the <code>random_state</code> parameter has no effect.</p> </li> </ol>"},{"location":"model_selection/train_test_split/#use-cases","title":"Use Cases","text":"<ul> <li>Splitting datasets into training and testing subsets for model evaluation.</li> <li>Creating validation sets for hyperparameter tuning.</li> <li>Ensuring consistent splits for debugging and reproducibility.</li> </ul>"},{"location":"neighbors/KNeighborsClassifier/","title":"KNeighborsClassifier","text":"<p>bolt &gt; neighbors &gt; _KNeighborsClassifier</p>"},{"location":"neighbors/KNeighborsClassifier/#kneighborsclassifier","title":"KNeighborsClassifier","text":"<p>The <code>KNeighborsClassifier</code> is a classic machine learning algorithm used for classification tasks. This model leverages the concept of \"proximity\" by identifying the <code>k</code> closest data points to a given input and assigning the most frequent label among these neighbors. It's especially effective for problems where similar data points tend to belong to the same class.</p>"},{"location":"neighbors/KNeighborsClassifier/#parameters","title":"Parameters","text":""},{"location":"neighbors/KNeighborsClassifier/#knnfitx-listlistf24-y-listf24-n_neighbors-u24-p-f24","title":"<code>Knn/fit(X: List[List[f24]], y: List[f24], n_neighbors: u24, p: f24)</code>","text":"<ul> <li>X: Training data, a list of lists where each sublist represents a feature vector of type <code>f24</code>.</li> <li>y: Target values, list of labels associated with each feature vector in <code>X</code>, type <code>f24</code>.</li> <li>n_neighbors: The number of neighbors to consider for classification (k value), type <code>u24</code>.</li> <li>p: The power parameter for the Minkowski metric. When <code>p = 1</code>, this uses the Manhattan distance (L1), and when <code>p = 2</code>, it uses the Euclidean distance (L2). Type <code>f24</code>.</li> </ul> <p>Returns: A fitted <code>KNeighborsClassifier</code> instance.</p>"},{"location":"neighbors/KNeighborsClassifier/#knnpredictmodel-kneighborsclassifier-x_test-listlistf24","title":"<code>Knn/predict(model: KNeighborsClassifier, X_test: List[List[f24]])</code>","text":"<ul> <li>model: A fitted <code>KNeighborsClassifier</code> instance.</li> <li>X_test: Test data, a list of lists where each sublist represents a feature vector, type <code>f24</code>.</li> </ul> <p>Returns: Predicted class labels for each data sample in <code>X_test</code>, as an array with shape <code>(n_queries,)</code>.</p>"},{"location":"neighbors/KNeighborsClassifier/#example-usage","title":"Example Usage","text":"<pre><code>X = [[5.0, 3.0], [3.0, 2.0], [1.5, 9.0], [7.0, 2.0]]\ny = [0.0, 1.0, 0.0, 1.0]\nk = 5\np = 2.0\n\n# Fit the KNeighborsClassifier model\nmodel = Knn/fit(X, y, k, p)\n\n# Define test data\nX_test = [[2.0, 7.0], [5.0, 3.0]]\n\n# Predict class labels for test data\ny_pred = Knn/predict(model, X_test)\n</code></pre>"},{"location":"neighbors/KNeighborsClassifier/#tuning-tips","title":"Tuning Tips","text":"<ul> <li>Choosing <code>k</code>: Larger <code>k</code> values reduce sensitivity to noise but may smooth out decision boundaries. Typical values are between 3 and 10.</li> <li>Choosing <code>p</code>: Experiment with <code>p=1</code> and <code>p=2</code> initially to see which metric works best. Higher values may result in less interpretable metrics.</li> </ul>"},{"location":"neighbors/KNeighborsRegressor/","title":"KNeighborsRegressor","text":"<p>bolt &gt; neighbors &gt; _KNeighborsRegressor</p>"},{"location":"neighbors/KNeighborsRegressor/#kneighborsregressor","title":"KNeighborsRegressor","text":"<p>The <code>KNeighborsRegressor</code> provides a regression algorithm based on the k-nearest neighbors approach. For each input, it identifies the <code>k</code> closest data points in the training set and predicts the output by averaging their values.</p>"},{"location":"neighbors/KNeighborsRegressor/#parameters","title":"Parameters","text":""},{"location":"neighbors/KNeighborsRegressor/#knnfitx-listlistf24-y-listf24-n_neighbors-u24-p-f24","title":"<code>Knn/fit(X: List[List[f24]], y: List[f24], n_neighbors: u24, p: f24)</code>","text":"<ul> <li>X: Training data, a list of lists where each sublist represents a feature vector of type <code>f24</code>.</li> <li>y: Target values, a list of numeric values associated with each feature vector in X, type f24..</li> <li>n_neighbors: The number of neighbors to consider for classification (k value), type <code>u24</code>.</li> <li>p: The power parameter for the Minkowski metric. When <code>p = 1</code>, this uses the Manhattan distance (L1), and when <code>p = 2</code>, it uses the Euclidean distance (L2). Type <code>f24</code>.</li> </ul> <p>Returns: A fitted <code>KNeighborsRegressor</code> instance.</p>"},{"location":"neighbors/KNeighborsRegressor/#knnpredictmodel-kneighborsregressor-x_test-listlistf24","title":"<code>Knn/predict(model: KNeighborsRegressor, X_test: List[List[f24]])</code>","text":"<ul> <li>model: A fitted <code>KNeighborsRegressor</code> instance.</li> <li>X_test: Test data, a list of lists where each sublist represents a feature vector, type <code>f24</code>.</li> </ul> <p>Returns: Predicted continuous values for each data sample in X_test, as an array with shape (n_queries,).</p>"},{"location":"neighbors/KNeighborsRegressor/#example-usage","title":"Example Usage","text":"<pre><code>X = [[5.0, 3.0], [3.0, 2.0], [1.5, 9.0], [7.0, 2.0]]\ny = [0.0, 1.0, 0.0, 1.0]\nk = 5\np = 2.0\n\n# Fit the KNeighborsRegressor model\nmodel = Knn/fit(X, y, k, p)\n\n# Define test data\nX_test = [[2.0, 7.0], [5.0, 3.0]]\n\n# Predict class labels for test data\ny_pred = Knn/predict(model, X_test)\n</code></pre>"},{"location":"neighbors/KNeighborsRegressor/#tuning-tips","title":"Tuning Tips","text":"<ul> <li>Choosing <code>k</code>: Larger <code>k</code> values reduce sensitivity to noise but may smooth out decision boundaries. Typical values are between 3 and 10.</li> <li>Choosing <code>p</code>: Experiment with <code>p=1</code> and <code>p=2</code> initially to see which metric works best. Higher values may result in less interpretable metrics.</li> </ul>"},{"location":"preprocessing/LabelEncoder/","title":"LabelEncoder","text":"<p>bolt &gt; preprocessing &gt; LabelEncoder</p>"},{"location":"preprocessing/LabelEncoder/#labelencoder","title":"LabelEncoder","text":"<p>The <code>LabelEncoder</code> class encodes categorical string labels as unique integer values, allowing categorical data to be used in numerical models. It also provides functionality to reverse the encoding and retrieve the original string labels.</p>"},{"location":"preprocessing/LabelEncoder/#overview","title":"Overview","text":"<p>Label encoding is a preprocessing step that converts categorical labels (strings) into numerical values, which is essential for many machine learning algorithms. Each unique label is assigned a unique integer, making it easy to handle categorical variables.</p>"},{"location":"preprocessing/LabelEncoder/#parameters","title":"Parameters","text":""},{"location":"preprocessing/LabelEncoder/#labelencoderfity-liststr","title":"<code>LabelEncoder/fit(y: List[str])</code>","text":"<ul> <li>y: Target labels, a list of categorical values in string format.</li> </ul> <p>Returns: A fitted <code>LabelEncoder</code> model, which stores mappings between classes and their integer encodings.</p>"},{"location":"preprocessing/LabelEncoder/#labelencodertransformmodel-labelencoder-y-liststr","title":"<code>LabelEncoder/transform(model: LabelEncoder, y: List[str])</code>","text":"<ul> <li>model: A fitted <code>LabelEncoder</code> instance.</li> <li>y: List of categorical values (strings) to encode, which should only include labels known to the model.</li> </ul> <p>Returns: Encoded labels as a list of integers (<code>u24</code>) corresponding to each categorical label in <code>y</code>.</p>"},{"location":"preprocessing/LabelEncoder/#labelencoderinverse_transformmodel-labelencoder-y-listu24","title":"<code>LabelEncoder/inverse_transform(model: LabelEncoder, y: List[u24])</code>","text":"<ul> <li>model: A fitted <code>LabelEncoder</code> instance.</li> <li>y: List of integer-encoded labels to decode back into the original string labels.</li> </ul> <p>Returns: Decoded labels as a list of strings corresponding to the original labels.</p>"},{"location":"preprocessing/LabelEncoder/#model-attributes","title":"Model Attributes","text":"<p>After fitting the model using <code>LabelEncoder/fit</code>, the following attributes become available:</p> <ul> <li>classes: A dictionary mapping each unique string label to its corresponding integer encoding (<code>u24</code>). Useful for understanding which integer values correspond to which labels.</li> <li>inverse_classes: A dictionary mapping each integer encoding (<code>u24</code>) back to its original string label. Useful for reversing transformations.</li> </ul>"},{"location":"preprocessing/LabelEncoder/#accessing-model-attributes","title":"Accessing Model Attributes","text":"<p>After the <code>LabelEncoder</code> model is fitted, you can inspect the mappings to understand how the labels are encoded:</p> <pre><code>model = LabelEncoder/fit(y)\n\n# Accessing encoded class mappings\nopen LabelEncoder: model\nwith IO:\n  IO/print(\"Classes:\", model.classes)\n  IO/print(\"Inverse Classes:\", model.inverse_classes)\n</code></pre>"},{"location":"preprocessing/LabelEncoder/#examples","title":"Examples","text":""},{"location":"preprocessing/LabelEncoder/#basic-usage","title":"Basic Usage","text":"<pre><code># Sample categorical labels\ny = [\"apple\", \"banana\", \"cherry\", \"apple\", \"banana\"]\n\n# Fit the LabelEncoder model to data\nmodel = LabelEncoder/fit(y)\n\n# Transform the labels to integer encoding\nencoded_y = LabelEncoder/transform(model, y)\nprint(\"Encoded Labels:\", encoded_y)\n# Output: Encoded Labels: [0, 1, 2, 0, 1]\n\n# Inverse transform the encoded labels to retrieve original labels\ndecoded_y = LabelEncoder/inverse_transform(model, encoded_y)\nprint(\"Decoded Labels:\", decoded_y)\n# Output: Decoded Labels: [\"apple\", \"banana\", \"cherry\", \"apple\", \"banana\"]\n</code></pre>"},{"location":"preprocessing/LabelEncoder/#explanation-of-encoding-and-decoding","title":"Explanation of Encoding and Decoding","text":"<ul> <li>Encoding: Converts categorical labels to unique integers, enabling algorithms that require numerical input.</li> <li>Decoding: Reverses the encoding, returning integers back to their original categorical labels. This is helpful for interpreting model outputs when the encoded labels are used in predictions.</li> </ul>"},{"location":"preprocessing/LabelEncoder/#common-issues-and-error-handling","title":"Common Issues and Error Handling","text":"<ul> <li>Unknown Labels in Transform: Attempting to transform a label that was not present in the training set will result in an error. Make sure all labels in <code>y</code> for <code>transform</code> are known.</li> <li>Incompatible Data in Inverse Transform: Ensure that integer values passed to <code>inverse_transform</code> were previously generated by <code>transform</code>. Passing out-of-range integers may lead to undefined results.</li> </ul>"},{"location":"preprocessing/StandardScaler/","title":"StandardScaler","text":"<p>bolt &gt; preprocessing &gt; _StandardScaler</p>"},{"location":"preprocessing/StandardScaler/#standardscaler","title":"StandardScaler","text":"<p>The <code>StandardScaler</code> class provides methods for scaling features to a standard normal distribution. It is commonly used to standardize features in data preprocessing, transforming the data to have a mean of 0 and a standard deviation of 1.</p>"},{"location":"preprocessing/StandardScaler/#overview","title":"Overview","text":"<p>Standard scaling is a crucial step in many machine learning pipelines, as it ensures that features have consistent scale, which can help gradient-based algorithms converge more quickly. The <code>StandardScaler</code> computes the mean and standard deviation of each feature and applies transformations to make the data conform to a normal distribution. Here\u2019s the mathematical formula for standard scaling, which can be added to the documentation in Markdown format:</p>"},{"location":"preprocessing/StandardScaler/#standard-scaling-formula","title":"Standard Scaling Formula","text":"<p>The <code>StandardScaler</code> transforms each feature \\(x_i\\) in the dataset to have a mean of 0 and a standard deviation of 1. The scaling formula is given by:</p> \\[ x_{\\text{scaled}} = \\frac{x - \\mu}{\\sigma} \\] <p>where: - \\(x\\) is the original feature value. - \\(\\mu\\) is the mean of the feature across all samples. - \\(\\sigma\\) is the standard deviation of the feature.</p> <p>For inverse scaling, we use:</p> \\[ x_{\\text{original}} = x_{\\text{scaled}} \\times \\sigma + \\mu \\] <p>This formula allows us to revert the scaled data back to its original values.</p> <p>This should give users a clearer understanding of the standard scaling and inverse scaling transformations.</p>"},{"location":"preprocessing/StandardScaler/#parameters","title":"Parameters","text":""},{"location":"preprocessing/StandardScaler/#standardscalerfitx-listlistf24","title":"<code>StandardScaler/fit(x: List[List[f24]])</code>","text":"<ul> <li>x: Input data, a list of lists where each sublist represents a feature vector, type <code>f24</code>.</li> </ul> <p>Returns: A fitted <code>StandardScaler</code> model, containing computed statistics (mean and standard deviation) for each feature.</p>"},{"location":"preprocessing/StandardScaler/#standardscalertransformmodel-standardscaler-x-listlistf24","title":"<code>StandardScaler/transform(model: StandardScaler, x: List[List[f24]])</code>","text":"<ul> <li>x: Data to transform, a list of lists where each sublist represents a feature vector, type <code>f24</code>.</li> <li>model: A fitted <code>StandardScaler</code> instance.</li> </ul> <p>Returns: Scaled data, a list of lists in the same shape as <code>x</code>.</p>"},{"location":"preprocessing/StandardScaler/#standardscalerinverse_transformmodel-standardscaler-x-listlistf24","title":"<code>StandardScaler/inverse_transform(model: StandardScaler, x: List[List[f24]])</code>","text":"<ul> <li>x: Data to transform, a list of lists where each sublist represents a encoded vector, type <code>f24</code>.</li> <li>model: A fitted <code>StandardScaler</code> instance.</li> </ul> <p>Returns: Inverserly scaled data, a list of lists in the same shape as <code>x</code>.</p>"},{"location":"preprocessing/StandardScaler/#model-attributes","title":"Model Attributes","text":"<p>After fitting the model using <code>standard_scaler</code>, the following attributes become available:</p> <ul> <li>featureMean: A list of mean values, one per feature, computed from the input data. Used as the central tendency for scaling.</li> <li>stddev: A list of standard deviation values, one per feature, computed from the input data. Used to scale each feature to a standard deviation of 1.</li> </ul>"},{"location":"preprocessing/StandardScaler/#accessing-model-attributes","title":"Accessing Model Attributes","text":"<p>After the <code>StandardScaler</code> model is fitted, you can access these attributes to understand the computed statistics:</p> <pre><code>model = standard_scaler(x)\n\n# Accessing model mean and standard deviation\nopen StandardScaler: model\nwith IO:\n  IO/print(\"Mean:\", model.featureMean)\n  IO/print(\"Standard Deviation:\", model.stddev)\n</code></pre>"},{"location":"preprocessing/StandardScaler/#examples","title":"Examples","text":""},{"location":"preprocessing/StandardScaler/#basic-usage","title":"Basic Usage","text":"<pre><code># Sample data\nx = [\n    [1.0, 2.0],\n    [10.0, 5.0]\n]\n\n# Fit the Standard Scaler model to data\nmodel = StandardScaler/fit(x)\nopen StandardScaler: model\n\n# Transform the data using standard scaling\nres = StandardScaler/transform(model, x)\nprint(\"Standard Scaled Data:\", res)\n\n# Inverse transform the scaled data to return to original scale\nout = StandardScaler/inverse_transform(model, res)\nprint(\"Inversely Scaled Data (Original Data):\", out)\n\n# Outputs:\n# Standard Scaled Data: [[-1.0, -1.0], [1.0, 1.0]]\n# Inversely Scaled Data (Original Data): [[1.0, 2.0], [10.0, 5.0]]\n</code></pre>"},{"location":"preprocessing/StandardScaler/#common-issues-and-error-handling","title":"Common Issues and Error Handling","text":"<ul> <li>Unfitted Model: Attempting to transform data with an unfitted model will result in errors. Ensure you run <code>standard_scaler(x)</code> first.</li> <li>Feature Scaling for Gradient-Based Algorithms: For gradient-based algorithms, it's recommended to use standard scaling as it helps prevent features with larger scales from dominating the learning process.</li> </ul>"},{"location":"tree/DecisionTreeClassifier/","title":"DecisionTreeClassifier","text":"<p>bolt &gt; trees &gt; _decisionTree</p>"},{"location":"tree/DecisionTreeClassifier/#decisiontree","title":"DecisionTree","text":"<p>The <code>DecisionTree</code> class implements a decision tree classifier, which recursively splits data based on feature values to create a tree structure. The model can then use this tree structure to classify new samples by following the decision paths from root to leaf.</p>"},{"location":"tree/DecisionTreeClassifier/#overview","title":"Overview","text":"<p>Decision trees are intuitive models for classification tasks. They work by creating a series of binary splits on features, aiming to maximize the separation between classes at each step. This results in a tree of decision nodes and leaf nodes representing final classifications. </p> <p>A key metric often used in decision trees is entropy, which measures the impurity or uncertainty in a dataset. </p>"},{"location":"tree/DecisionTreeClassifier/#entropy-and-information-gain","title":"Entropy and Information Gain","text":"<p>In decision trees, entropy quantifies the impurity in the data at each node. Lower entropy indicates a purer node (i.e., a node where most data points belong to a single class). The tree-building process uses information gain, calculated from entropy, to decide which feature to split on at each node.</p>"},{"location":"tree/DecisionTreeClassifier/#entropy-formula","title":"Entropy Formula","text":"<p>For a set of classes \\(C = \\{c_1, c_2, \\ldots, c_k\\}\\), where \\(p(c_i)\\) is the proportion of data points in class \\(c_i\\) at a given node, the entropy \\(H\\) at that node is defined as:</p> \\[ H(C) = - \\sum_{i=1}^{k} p(c_i) \\cdot \\log_2(p(c_i)) \\] <p>where: - \\(p(c_i)\\) is the probability of class \\(c_i\\) within the node. - \\(\\log_2\\) is the base-2 logarithm.  </p> <p>A node with only one class will have an entropy of 0 (completely pure), while a node with an equal mix of classes will have the highest entropy.</p>"},{"location":"tree/DecisionTreeClassifier/#information-gain","title":"Information Gain","text":"<p>When a split is made based on a feature, information gain calculates the reduction in entropy. Higher information gain means the split yields a purer separation between classes. It\u2019s given by:</p> \\[ \\text{Information Gain} = H(\\text{parent}) - \\left( \\frac{n_{\\text{left}}}{n} \\cdot H(\\text{left}) + \\frac{n_{\\text{right}}}{n} \\cdot H(\\text{right}) \\right) \\] <p>where: - \\(H(\\text{parent})\\) is the entropy of the parent node. - \\(H(\\text{left})\\) and \\(H(\\text{right})\\) are the entropies of the left and right child nodes. - \\(n_{\\text{left}}\\) and \\(n_{\\text{right}}\\) are the number of samples in the left and right child nodes. - \\(n `\\) is the total number of samples in the parent node.  </p>"},{"location":"tree/DecisionTreeClassifier/#parameters","title":"Parameters","text":""},{"location":"tree/DecisionTreeClassifier/#decisiontreefitx-listlistf24-y-listf24-max_depth-u24-min_samples_split-u24","title":"<code>DecisionTree/fit(X: List[List[f24]], y: List[f24], max_depth: u24, min_samples_split: u24)</code>","text":"<ul> <li>X: Training data, a list of lists where each sublist represents a feature vector, type <code>f24</code>.</li> <li>y: Target values, a list of labels corresponding to each feature vector in <code>X</code>, type <code>f24</code>.</li> <li>max_depth: Maximum depth of the tree. Limits the number of splits to prevent overfitting, type <code>u24</code>.</li> <li>min_samples_split: Minimum number of samples required to split a node. Restricts splits on small nodes to avoid overfitting, type <code>u24</code>.</li> </ul> <p>Returns: A fitted <code>DecisionTree</code> model.</p>"},{"location":"tree/DecisionTreeClassifier/#decisiontreepredictmodel-decisiontree-x-listlistf24","title":"<code>DecisionTree/predict(model: DecisionTree, x: List[List[f24]])</code>","text":"<ul> <li>model: A fitted <code>DecisionTree</code> instance.</li> <li>x: Input data for prediction, a list of lists where each sublist represents a feature vector, type <code>f24</code>.</li> </ul> <p>Returns: Predicted class labels for each data sample in <code>x</code>.</p>"},{"location":"tree/DecisionTreeClassifier/#model-attributes","title":"Model Attributes","text":"<p>After fitting the model using <code>DecisionTree/fit</code>, the following attributes define the structure of the tree:</p> <ul> <li>Node: Represents an internal decision node in the tree with the following attributes:</li> <li>feature: The index of the feature used for splitting at this node.</li> <li>threshold: The value of the feature at which the split occurs.</li> <li>~left: Reference to the left child node, where samples meeting the split condition are sent.</li> <li> <p>~right: Reference to the right child node, where samples not meeting the split condition are sent.</p> </li> <li> <p>Leaf: Represents a terminal leaf node with the following attribute:</p> </li> <li>value: The predicted class label or probability for samples reaching this leaf.</li> </ul>"},{"location":"tree/DecisionTreeClassifier/#accessing-model-attributes","title":"Accessing Model Attributes","text":"<p>Once the model is trained, you can traverse the tree\u2019s structure to inspect the decision nodes and leaf nodes:</p> <pre><code>model = DecisionTree/fit(X, y, max_depth, min_samples_split)\n\nwith IO:\n    match model:\n        case DecisionTree/Node:\n            IO/print(\"Root Node Feature:\", model.feature)\n            IO/print(\"Root Node Threshold:\", model.threshold)\n        case DecisionTree/Leaf:\n            IO/print(\"Root Node Feature:\", model.value)\n</code></pre>"},{"location":"tree/DecisionTreeClassifier/#examples","title":"Examples","text":""},{"location":"tree/DecisionTreeClassifier/#basic-usage","title":"Basic Usage","text":"<pre><code># Training data\nX = [\n    [2.5, 3.1],\n    [1.2, 7.8],\n    [3.3, 6.5],\n    [1.7, 2.8],\n    [3.1, 5.4],\n    [4.0, 1.2],\n]\ny = [0.0, 1.0, 1.0, 0.0, 1.0, 0.0]\n\n# Decision Tree parameters\nmax_depth = 3\nmin_samples_split = 2\n\n# Fit the Decision Tree model\nmodel = DecisionTree/fit(X, y, max_depth, min_samples_split)\n\n# Define test data for prediction\nx_test = [\n    [3.0, 4.0],\n    [1.5, 6.0]\n]\n\n# Predict class labels for test data\ny_pred = DecisionTree/predict(model, x_test)\n\nprint(\"Predicted Class Labels:\", y_pred)\n# Output: Predicted Class Labels: [predicted_label1, predicted_label2]\n</code></pre>"},{"location":"tree/DecisionTreeClassifier/#explanation-of-tree-parameters","title":"Explanation of Tree Parameters","text":"<ul> <li>max_depth: Limits the depth of the tree to prevent overfitting. Shallow trees may underfit, while deep trees may overfit to the training data.</li> <li>min_samples_split: Ensures each node has a minimum number of samples before being split. Higher values create broader splits, reducing overfitting.</li> </ul>"},{"location":"tree/DecisionTreeClassifier/#tree-structure-and-traversal","title":"Tree Structure and Traversal","text":"<p>Each decision node in the tree splits data based on a feature value:  </p> \\[ \\text{if } x_{\\text{feature}} \\leq \\text{threshold, go to left child; else, go to right child.} \\] <p>Once a sample reaches a leaf node, the leaf node\u2019s value is used as the predicted class for that sample.</p>"},{"location":"tree/DecisionTreeClassifier/#common-issues-and-error-handling","title":"Common Issues and Error Handling","text":"<ul> <li>Overfitting: High <code>max_depth</code> or low <code>min_samples_split</code> may cause overfitting, capturing noise in the training data.</li> <li>Underfitting: Low <code>max_depth</code> or high <code>min_samples_split</code> may cause underfitting, missing important patterns in the data.</li> <li>Imbalanced Classes: For imbalanced data, consider limiting tree depth or applying class weights to ensure fair representation.</li> </ul>"}]}